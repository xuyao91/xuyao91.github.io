
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>使用PyTorch构建CNN网络训练FashionMNIST数据集 - 老徐</title>
  <meta name="author" content="Peter Xu">

  
  <meta name="description" content="前言 本文记录了使用PyTorch构建一个简单的CNN网络，并使用Fahion-MNIST数据集进行训练和测试。记录整个推理过程 1、Fashion-MNIST数据集 Fashion-MNIST是一个流行的图像分类数据集，用于机器学习和计算机视觉任务。数据集包含了来自10个不同类别的70, &hellip;">
  

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  
  <link rel="canonical" href="http://xuyao.club/blog/2023/07/27/pytorch-was-used-to-build-a-cnn-network-to-train-the-fashion-mnist-dataset/">
  <link href="/favicon.png" rel="icon">
  <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <link href="/atom.xml" rel="alternate" title="老徐" type="application/atom+xml">
  <script src="/javascripts/modernizr-2.0.js"></script>
  <script src="http://cdn.staticfile.org/jquery/1.9.1/jquery.min.js"></script>
  <script>!window.jQuery && document.write(unescape('%3Cscript src="/javascripts/libs/jquery.min.js"%3E%3C/script%3E'))</script>
  <script src="/javascripts/octopress.js" type="text/javascript"></script>
  <!--Fonts from Google"s Web font directory at http://google.com/webfonts -->
<link href="//fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">
<link href="//fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic" rel="stylesheet" type="text/css">

  

</head>

<body   >
  <header role="banner"><hgroup>
  <h1><a href="/">老徐</a></h1>
  
    <h2>Never underestimate your power to change yourself!</h2>
  
</hgroup>

</header>
  <nav role="navigation"><ul class="subscription" data-subscription="rss">
  <li><a href="/atom.xml" rel="subscribe-rss" title="subscribe via RSS">RSS</a></li>
  
</ul>
  
<form action="https://cn.bing.com" method="get">
  <fieldset role="search">
    <input type="hidden" name="sitesearch" value="xuyao.club">
    <input class="search" type="text" name="q" results="0" placeholder="Search"/>
  </fieldset>
</form>
  
<ul class="main-navigation">
  <li><a href="/">Blog</a></li>
  <li><a href="/blog/archives">Archives</a></li>
  <li><a href="/blog/2015/06/05/about-me-with-ruby-code/">About Me</a></li>
</ul>

</nav>
  <div id="main">
    <div id="content">
      <div>
<article class="hentry" role="article">
  
  <header>
    
      <h1 class="entry-title">使用PyTorch构建CNN网络训练FashionMNIST数据集</h1>
    
    
      <p class="meta">
        




<time class='entry-date' datetime='2023-07-27T16:31:49+08:00'><span class='date'><span class='date-month'>Jul</span> <span class='date-day'>27</span><span class='date-suffix'>th</span>, <span class='date-year'>2023</span></span> <span class='time'>4:31 pm</span></time>
        
        <!-- doushuo commnet -->
        
          | <a href="#comments">Comments</a>
         
      </p>
    
  </header>


<div class="entry-content"><h4>前言</h4>

<p>本文记录了使用PyTorch构建一个简单的CNN网络，并使用Fahion-MNIST数据集进行训练和测试。记录整个推理过程</p>

<!--more-->


<h4>1、Fashion-MNIST数据集</h4>

<p>Fashion-MNIST是一个流行的图像分类数据集，用于机器学习和计算机视觉任务。数据集包含了来自10个不同类别的70,000个灰度图像。每个图像的分辨率为28x28像素，并包含单件服装或配饰的图像。这些类别包括：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>1、T恤
</span><span class='line'>2、裤子
</span><span class='line'>3、毛衣
</span><span class='line'>4、裙子
</span><span class='line'>5、外套
</span><span class='line'>6、凉鞋
</span><span class='line'>7、衬衫
</span><span class='line'>8、运动鞋
</span><span class='line'>9、包
</span><span class='line'>10、短靴</span></code></pre></td></tr></table></div></figure>


<p>Fashion-MNIST数据集的目标是训练机器学习模型来准确地识别图像中的服装类别。由于这些图像是灰度图像，每个像素的值介于0到255之间。</p>

<p>该数据集通常用于测试图像分类算法的性能，以及在实践中验证新的计算机视觉技术。它与MNIST数据集类似，但更具挑战性，因为服装图像的变化更加复杂。</p>

<h4>2、构建卷积神经网络</h4>

<p>由于Fashion-MINST数据集跟MINST数据集一样，都是28*28的灰度图片，所以他们的输入是一样的，我们可以直接按照上次训练MINST的卷积神经网络来训练Fashion-MINST，上一讲中训练MINST的网络结构如下(具体可参考上一篇文章)：
Conv1->Relu->pool->Conv2->Relu->pool->Linear1->Relu->Linear2->Sofrmax
使用上一讲的卷积神经网络训练，训练10个epoch后的准确率在86%左右，可以看出，准确率不是很高，现在改一下神经网络，来提高其准确率，主要是增加卷积层，上一讲中只有两个卷积层，现在我们可以再增加一个卷积层，来看下训练效果，代码如下：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
<span class='line-number'>48</span>
<span class='line-number'>49</span>
<span class='line-number'>50</span>
<span class='line-number'>51</span>
<span class='line-number'>52</span>
<span class='line-number'>53</span>
<span class='line-number'>54</span>
<span class='line-number'>55</span>
<span class='line-number'>56</span>
<span class='line-number'>57</span>
<span class='line-number'>58</span>
<span class='line-number'>59</span>
<span class='line-number'>60</span>
<span class='line-number'>61</span>
<span class='line-number'>62</span>
<span class='line-number'>63</span>
<span class='line-number'>64</span>
<span class='line-number'>65</span>
<span class='line-number'>66</span>
<span class='line-number'>67</span>
<span class='line-number'>68</span>
<span class='line-number'>69</span>
<span class='line-number'>70</span>
<span class='line-number'>71</span>
<span class='line-number'>72</span>
<span class='line-number'>73</span>
<span class='line-number'>74</span>
<span class='line-number'>75</span>
<span class='line-number'>76</span>
<span class='line-number'>77</span>
<span class='line-number'>78</span>
<span class='line-number'>79</span>
<span class='line-number'>80</span>
<span class='line-number'>81</span>
<span class='line-number'>82</span>
<span class='line-number'>83</span>
<span class='line-number'>84</span>
<span class='line-number'>85</span>
<span class='line-number'>86</span>
<span class='line-number'>87</span>
<span class='line-number'>88</span>
<span class='line-number'>89</span>
<span class='line-number'>90</span>
<span class='line-number'>91</span>
<span class='line-number'>92</span>
<span class='line-number'>93</span>
<span class='line-number'>94</span>
<span class='line-number'>95</span>
<span class='line-number'>96</span>
<span class='line-number'>97</span>
<span class='line-number'>98</span>
<span class='line-number'>99</span>
<span class='line-number'>100</span>
<span class='line-number'>101</span>
<span class='line-number'>102</span>
<span class='line-number'>103</span>
<span class='line-number'>104</span>
<span class='line-number'>105</span>
<span class='line-number'>106</span>
<span class='line-number'>107</span>
<span class='line-number'>108</span>
<span class='line-number'>109</span>
<span class='line-number'>110</span>
<span class='line-number'>111</span>
<span class='line-number'>112</span>
<span class='line-number'>113</span>
<span class='line-number'>114</span>
<span class='line-number'>115</span>
<span class='line-number'>116</span>
<span class='line-number'>117</span>
<span class='line-number'>118</span>
<span class='line-number'>119</span>
<span class='line-number'>120</span>
<span class='line-number'>121</span>
<span class='line-number'>122</span>
<span class='line-number'>123</span>
<span class='line-number'>124</span>
<span class='line-number'>125</span>
<span class='line-number'>126</span>
<span class='line-number'>127</span>
<span class='line-number'>128</span>
<span class='line-number'>129</span>
<span class='line-number'>130</span>
<span class='line-number'>131</span>
<span class='line-number'>132</span>
<span class='line-number'>133</span>
<span class='line-number'>134</span>
<span class='line-number'>135</span>
<span class='line-number'>136</span>
<span class='line-number'>137</span>
<span class='line-number'>138</span>
<span class='line-number'>139</span>
<span class='line-number'>140</span>
<span class='line-number'>141</span>
<span class='line-number'>142</span>
<span class='line-number'>143</span>
<span class='line-number'>144</span>
<span class='line-number'>145</span>
<span class='line-number'>146</span>
<span class='line-number'>147</span>
<span class='line-number'>148</span>
<span class='line-number'>149</span>
<span class='line-number'>150</span>
<span class='line-number'>151</span>
<span class='line-number'>152</span>
<span class='line-number'>153</span>
<span class='line-number'>154</span>
<span class='line-number'>155</span>
<span class='line-number'>156</span>
<span class='line-number'>157</span>
<span class='line-number'>158</span>
<span class='line-number'>159</span>
<span class='line-number'>160</span>
<span class='line-number'>161</span>
<span class='line-number'>162</span>
<span class='line-number'>163</span>
<span class='line-number'>164</span>
<span class='line-number'>165</span>
<span class='line-number'>166</span>
<span class='line-number'>167</span>
<span class='line-number'>168</span>
<span class='line-number'>169</span>
<span class='line-number'>170</span>
<span class='line-number'>171</span>
<span class='line-number'>172</span>
<span class='line-number'>173</span>
<span class='line-number'>174</span>
<span class='line-number'>175</span>
<span class='line-number'>176</span>
<span class='line-number'>177</span>
<span class='line-number'>178</span>
<span class='line-number'>179</span>
<span class='line-number'>180</span>
<span class='line-number'>181</span>
<span class='line-number'>182</span>
<span class='line-number'>183</span>
<span class='line-number'>184</span>
<span class='line-number'>185</span>
<span class='line-number'>186</span>
<span class='line-number'>187</span>
<span class='line-number'>188</span>
<span class='line-number'>189</span>
<span class='line-number'>190</span>
<span class='line-number'>191</span>
<span class='line-number'>192</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>import torch
</span><span class='line'>import torch.nn as nn
</span><span class='line'>import torch.nn.functional as F
</span><span class='line'>from torchvision import datasets, transforms
</span><span class='line'>import torchvision
</span><span class='line'>from torch.autograd import Variable
</span><span class='line'>import torch.optim as optim
</span><span class='line'>import matplotlib.pyplot as plt
</span><span class='line'>import sys
</span><span class='line'>
</span><span class='line'>BATCH_SIZE = 200 #每单次训练时加载的数据量，如果是用GPU跑的话，可以设置得高一点。
</span><span class='line'>EPOCHS = 5  # 总共训练批次
</span><span class='line'>DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")  # 让torch判断是否使用GPU，建议使用GPU环境，因为会快很多
</span><span class='line'>
</span><span class='line'>train_loader = torch.utils.data.DataLoader(
</span><span class='line'>    datasets.FashionMNIST('../datasets/data', train=True, download=True,
</span><span class='line'>                   transform=transforms.Compose([
</span><span class='line'>                       transforms.ToTensor(),
</span><span class='line'>                       transforms.Normalize(mean=[0.5],std=[0.5])  # transforms.Normalize()将数据进行归一化处理，
</span><span class='line'>                   ])),
</span><span class='line'>    batch_size=BATCH_SIZE, shuffle=True)
</span><span class='line'>
</span><span class='line'>test_loader = torch.utils.data.DataLoader(
</span><span class='line'>    datasets.FashionMNIST('../datasets/data', train=False, transform=transforms.Compose([
</span><span class='line'>        transforms.ToTensor(),
</span><span class='line'>        transforms.Normalize(mean=[0.5],std=[0.5])
</span><span class='line'>    ])),
</span><span class='line'>    batch_size=BATCH_SIZE, shuffle=True)
</span><span class='line'>
</span><span class='line'># Class labels
</span><span class='line'>classes = ('T恤', '牛仔裤', '毛衣', '裙子', '外套',
</span><span class='line'>        '凉鞋', '衬衫', '运动鞋', '包', '短靴')
</span><span class='line'>
</span><span class='line'># 网络结构 conv-&gt;Relu-&gt;pool-&gt;conv-&gt;Relu-&gt;pool-&gt;Linear-&gt;Relu-&gt;Linear-&gt;Sofrmax
</span><span class='line'>class MNISTConvNet(nn.Module):
</span><span class='line'>  def __init__(self):
</span><span class='line'>    super().__init__()
</span><span class='line'>    # 公式：OH=(H + 2P - FH)/S + 1; OW= (W + 2P - FW)/S + 1
</span><span class='line'>    self.conv1 = nn.Conv2d(in_channels=1, out_channels=24, kernel_size=3, stride=1, padding=0) # 12, 26*26
</span><span class='line'>    self.conv2 = nn.Conv2d(in_channels=24, out_channels=48, kernel_size=3, stride=1, padding=0) # 24, 24*24
</span><span class='line'>    self.conv3 = nn.Conv2d(in_channels=48, out_channels=96, kernel_size=3, stride=1, padding=0) # 48, 10*10
</span><span class='line'>    self.fc1 = nn.Linear(in_features=96*5*5, out_features=400)
</span><span class='line'>    self.fc2 = nn.Linear(in_features=400, out_features=10)
</span><span class='line'>    
</span><span class='line'>  def forward(self, x):
</span><span class='line'>    out = self.conv1(x)
</span><span class='line'>    out = F.relu(out)
</span><span class='line'>    # out = F.max_pool2d(input=out, kernel_size=(2,2), stride=2) #12 13*13
</span><span class='line'>    
</span><span class='line'>    out = self.conv2(out)
</span><span class='line'>    out = F.relu(out)
</span><span class='line'>    out = F.max_pool2d(input=out, kernel_size=(2,2), stride=2) #24 12*12
</span><span class='line'>    
</span><span class='line'>    out = self.conv3(out)
</span><span class='line'>    out = F.relu(out)
</span><span class='line'>    out = F.max_pool2d(input=out, kernel_size=(2,2), stride=2) #48 5*5
</span><span class='line'>    
</span><span class='line'>    out = out.view(-1,96*5*5)
</span><span class='line'>    out = self.fc1(out)
</span><span class='line'>    out = F.relu(out)
</span><span class='line'>    
</span><span class='line'>    out = self.fc2(out)
</span><span class='line'>    
</span><span class='line'>    out = F.log_softmax(out, dim=1)
</span><span class='line'>    return out
</span><span class='line'>
</span><span class='line'>def train(model, optimizer, epoch):
</span><span class='line'>  model.train() #设置模型为训练模式
</span><span class='line'>  train_loss = 0
</span><span class='line'>  correct = 0
</span><span class='line'>  for batch_id, (images, labels) in enumerate(train_loader):
</span><span class='line'>    images = images.to(DEVICE) # 将tensors移动到配置的device
</span><span class='line'>    labels = labels.to(DEVICE)
</span><span class='line'>    optimizer.zero_grad()  # 在反向传播的时候先把梯度记录清0
</span><span class='line'>    output = model(images)
</span><span class='line'>    loss = F.nll_loss(output, labels)
</span><span class='line'>    loss.backward()
</span><span class='line'>    optimizer.step()  # 调用step()方法更新梯度参数
</span><span class='line'>    if (batch_id + 1) % 30 == 0:
</span><span class='line'>        print(f'Train Epoch: {epoch} [{batch_id * len(images)}/{len(train_loader.dataset)} ({ 100. * batch_id / len(train_loader):.0f}%)]\tLoss: {loss.item()}')
</span><span class='line'>    train_loss += loss.item()
</span><span class='line'>    # print(f'train loss: {loss.item()}, train_loss_sum: {train_loss}')
</span><span class='line'>    
</span><span class='line'>    pred = output.max(1, keepdim=True)[1]  # 找到概率最大的下标
</span><span class='line'>    correct += pred.eq(labels.view_as(pred)).sum().item()
</span><span class='line'>  return 100. * train_loss / len(train_loader.dataset), 100. * correct / len(train_loader.dataset)
</span><span class='line'>  
</span><span class='line'>def test(model):
</span><span class='line'>  model.eval()
</span><span class='line'>  test_loss = 0
</span><span class='line'>  correct = 0
</span><span class='line'>  with torch.no_grad():  # 禁用梯度计算，验证模式一般不需要进行梯度更新及反向传播，所以不需要更新梯度参数
</span><span class='line'>      for images, labels in test_loader:
</span><span class='line'>          images, labels = images.to(DEVICE), labels.to(DEVICE)
</span><span class='line'>          output = model(images)
</span><span class='line'>          test_loss += F.nll_loss(output, labels, reduction='sum').item()  # 将一批的损失相加
</span><span class='line'>          pred = output.max(1, keepdim=True)[1]  # 找到概率最大的下标
</span><span class='line'>          correct += pred.eq(labels.view_as(pred)).sum().item()
</span><span class='line'>
</span><span class='line'>  test_loss /= len(test_loader.dataset)
</span><span class='line'>  print(f'\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} ({100. * correct / len(test_loader.dataset)}%)\n')
</span><span class='line'>  return test_loss, 100. * correct / len(test_loader.dataset)
</span><span class='line'>
</span><span class='line'>def predict(model):
</span><span class='line'>  val_loader = torch.utils.data.DataLoader(
</span><span class='line'>    datasets.FashionMNIST('../datasets/data', train=False, transform=transforms.Compose([
</span><span class='line'>                       transforms.ToTensor(),
</span><span class='line'>                       transforms.Normalize(mean=[0.5],std=[0.5])
</span><span class='line'>                   ])),batch_size=8, shuffle=True)
</span><span class='line'>  iterator = iter(val_loader)#获得Iterator对象:
</span><span class='line'>  X_val, Y_val = next(iterator) #循环Iterator对象迭代器，一般和上面的iter()一起使用
</span><span class='line'>  inputs = Variable(X_val)
</span><span class='line'>  #Variable()函数用于将一个张量转换为可训练的变量(也称为梯度变量)。使用Variable()函数可以方便地更新神经网络中的参数，从而实现反向传播算法。
</span><span class='line'>  pred = model(inputs)
</span><span class='line'>  _,pred = torch.max(pred, 1) #找到 tensor里最大的值，torch.max(input, dim)，input是softmax函数输出的一个tensor。 dim是max函数索引的维度0/1，0是每列的最大值，1是每行的最大值
</span><span class='line'>  print("实际结果:",[classes[i] for i in Y_val])
</span><span class='line'>  print("推理结果:", [ classes[i] for i in pred.data])
</span><span class='line'>
</span><span class='line'>  img = torchvision.utils.make_grid(X_val)
</span><span class='line'>  img = img.numpy().transpose(1,2,0)
</span><span class='line'>
</span><span class='line'>  std = [0.5,0.5,0.5] #标准差，用于计算标准化后的每个通道的值。
</span><span class='line'>  mean = [0.5,0.5,0.5] #均值，用于计算标准化后的每个通道的值。
</span><span class='line'>  img = img*std+mean
</span><span class='line'>  plt.imshow(img)
</span><span class='line'>  plt.show()
</span><span class='line'>  
</span><span class='line'>
</span><span class='line'>def run_train():
</span><span class='line'>  model = MNISTConvNet().to(DEVICE)
</span><span class='line'>  optimizer = optim.Adam(model.parameters()) #Adam方法更新权重参数
</span><span class='line'>  # optimizer = optim.SGD(model.parameters(), lr=1e-1, momentum = 0.9) #SGD方法更新权重参数,学习率设置大一点，过小梯度下降过慢
</span><span class='line'>  train_loss_list, test_loss_list = [],[]
</span><span class='line'>  train_acc_list, test_acc_list = [],[]
</span><span class='line'>  for epoch in range(1, EPOCHS + 1):
</span><span class='line'>    train_loss, train_acc =  train(model, optimizer, epoch)
</span><span class='line'>    #torch.save(model.state_dict(), 'fashion_mnist_model.pkl') #保存模型
</span><span class='line'>    test_loss, test_acc = test(model)
</span><span class='line'>    train_acc_list.append(train_acc)
</span><span class='line'>    test_acc_list.append(test_acc)
</span><span class='line'>    train_loss_list.append(train_loss)
</span><span class='line'>    test_loss_list.append(test_loss)
</span><span class='line'>    print(f'Epoch: {epoch} Train Loss: {train_loss:.4f}, Train Acc.: {train_acc:.2f}% | Validation Loss: {test_loss:.4f}, Validation Acc.: {test_acc:.2f}%')
</span><span class='line'>  
</span><span class='line'>  plt.plot(range(1, EPOCHS+1), train_loss_list, label='Training loss')
</span><span class='line'>  plt.plot(range(1, EPOCHS+1), test_loss_list, label='Validation loss')
</span><span class='line'>  plt.legend(loc='upper right')
</span><span class='line'>  plt.ylabel('Cross entropy')
</span><span class='line'>  plt.xlabel('Epoch')
</span><span class='line'>  plt.show()
</span><span class='line'>  
</span><span class='line'>
</span><span class='line'>def run_predict():
</span><span class='line'>  model = MNISTConvNet().to(DEVICE)
</span><span class='line'>  model.load_state_dict(torch.load('fashion_mnist_model.pkl')) #加载之前训练好的模型
</span><span class='line'>  predict(model)
</span><span class='line'>
</span><span class='line'>def show_image():
</span><span class='line'>  train_loader = torch.utils.data.DataLoader(
</span><span class='line'>    datasets.FashionMNIST('../datasets/data', train=True, download=True,
</span><span class='line'>                   transform=transforms.Compose([
</span><span class='line'>                       transforms.ToTensor(),
</span><span class='line'>                       transforms.Normalize((0.1307,), (0.3081,))  # transforms.Normalize()将数据进行归一化处理，
</span><span class='line'>                   ])),
</span><span class='line'>    batch_size=64, shuffle=True)
</span><span class='line'>  X_train, Y_train = next(iter(train_loader)) #循环Iterator对象迭代器，一般和上面的iter()一起使用
</span><span class='line'>  print("Image Label is:",[i for i in Y_train])
</span><span class='line'>
</span><span class='line'>  img = torchvision.utils.make_grid(X_train)
</span><span class='line'>  img = img.numpy().transpose(1,2,0)
</span><span class='line'>
</span><span class='line'>  std = [0.5,0.5,0.5] #标准差，用于计算标准化后的每个通道的值。
</span><span class='line'>  mean = [0.5,0.5,0.5] #均值，用于计算标准化后的每个通道的值。
</span><span class='line'>  img = img*std+mean
</span><span class='line'>  plt.imshow(img)
</span><span class='line'>  plt.show()
</span><span class='line'>
</span><span class='line'>if __name__ == '__main__':
</span><span class='line'>  if len(sys.argv) &lt;= 1:
</span><span class='line'>    print("请输入参数：train|predict|img") #train:表示训练数据，predict#表示推理，#img只是单纯地显示一下图片
</span><span class='line'>  else:
</span><span class='line'>    action = sys.argv[1]
</span><span class='line'>    if action == "train":
</span><span class='line'>      run_train() # 用于训练数据
</span><span class='line'>    elif action == "predict":
</span><span class='line'>      print("-----")
</span><span class='line'>      run_predict() # 用于预测图片
</span><span class='line'>    elif action == "img":
</span><span class='line'>      show_image() # 只是简单地显示图片
</span><span class='line'>    else:
</span><span class='line'>      print("请输入参数：train|predict|img")
</span><span class='line'>  
</span></code></pre></td></tr></table></div></figure>


<p>可以看到与上一讲的MINST网络的主要区别是增加一个卷积层。</p>

<h5>3、训练</h5>

<p>运行10epoch后的训练结果如下：</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
<span class='line-number'>3</span>
<span class='line-number'>4</span>
<span class='line-number'>5</span>
<span class='line-number'>6</span>
<span class='line-number'>7</span>
<span class='line-number'>8</span>
<span class='line-number'>9</span>
<span class='line-number'>10</span>
<span class='line-number'>11</span>
<span class='line-number'>12</span>
<span class='line-number'>13</span>
<span class='line-number'>14</span>
<span class='line-number'>15</span>
<span class='line-number'>16</span>
<span class='line-number'>17</span>
<span class='line-number'>18</span>
<span class='line-number'>19</span>
<span class='line-number'>20</span>
<span class='line-number'>21</span>
<span class='line-number'>22</span>
<span class='line-number'>23</span>
<span class='line-number'>24</span>
<span class='line-number'>25</span>
<span class='line-number'>26</span>
<span class='line-number'>27</span>
<span class='line-number'>28</span>
<span class='line-number'>29</span>
<span class='line-number'>30</span>
<span class='line-number'>31</span>
<span class='line-number'>32</span>
<span class='line-number'>33</span>
<span class='line-number'>34</span>
<span class='line-number'>35</span>
<span class='line-number'>36</span>
<span class='line-number'>37</span>
<span class='line-number'>38</span>
<span class='line-number'>39</span>
<span class='line-number'>40</span>
<span class='line-number'>41</span>
<span class='line-number'>42</span>
<span class='line-number'>43</span>
<span class='line-number'>44</span>
<span class='line-number'>45</span>
<span class='line-number'>46</span>
<span class='line-number'>47</span>
<span class='line-number'>48</span>
<span class='line-number'>49</span>
<span class='line-number'>50</span>
<span class='line-number'>51</span>
<span class='line-number'>52</span>
<span class='line-number'>53</span>
<span class='line-number'>54</span>
<span class='line-number'>55</span>
<span class='line-number'>56</span>
<span class='line-number'>57</span>
<span class='line-number'>58</span>
<span class='line-number'>59</span>
<span class='line-number'>60</span>
<span class='line-number'>61</span>
<span class='line-number'>62</span>
<span class='line-number'>63</span>
<span class='line-number'>64</span>
<span class='line-number'>65</span>
<span class='line-number'>66</span>
<span class='line-number'>67</span>
<span class='line-number'>68</span>
<span class='line-number'>69</span>
<span class='line-number'>70</span>
<span class='line-number'>71</span>
<span class='line-number'>72</span>
<span class='line-number'>73</span>
<span class='line-number'>74</span>
<span class='line-number'>75</span>
<span class='line-number'>76</span>
<span class='line-number'>77</span>
<span class='line-number'>78</span>
<span class='line-number'>79</span>
<span class='line-number'>80</span>
<span class='line-number'>81</span>
<span class='line-number'>82</span>
<span class='line-number'>83</span>
<span class='line-number'>84</span>
<span class='line-number'>85</span>
<span class='line-number'>86</span>
<span class='line-number'>87</span>
<span class='line-number'>88</span>
<span class='line-number'>89</span>
<span class='line-number'>90</span>
<span class='line-number'>91</span>
<span class='line-number'>92</span>
<span class='line-number'>93</span>
<span class='line-number'>94</span>
<span class='line-number'>95</span>
<span class='line-number'>96</span>
<span class='line-number'>97</span>
<span class='line-number'>98</span>
<span class='line-number'>99</span>
<span class='line-number'>100</span>
<span class='line-number'>101</span>
<span class='line-number'>102</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>Train Epoch: 1 [5800/60000 (10%)] Loss: 0.6379690766334534
</span><span class='line'>Train Epoch: 1 [11800/60000 (20%)]  Loss: 0.5845944881439209
</span><span class='line'>Train Epoch: 1 [17800/60000 (30%)]  Loss: 0.4599098265171051
</span><span class='line'>Train Epoch: 1 [23800/60000 (40%)]  Loss: 0.512351393699646
</span><span class='line'>Train Epoch: 1 [29800/60000 (50%)]  Loss: 0.40181490778923035
</span><span class='line'>Train Epoch: 1 [35800/60000 (60%)]  Loss: 0.3760054409503937
</span><span class='line'>Train Epoch: 1 [41800/60000 (70%)]  Loss: 0.36197417974472046
</span><span class='line'>Train Epoch: 1 [47800/60000 (80%)]  Loss: 0.3655475676059723
</span><span class='line'>Train Epoch: 1 [53800/60000 (90%)]  Loss: 0.40860098600387573
</span><span class='line'>Train Epoch: 1 [59800/60000 (100%)] Loss: 0.38368210196495056
</span><span class='line'>Train Epoch: 2 [5800/60000 (10%)] Loss: 0.3502006232738495
</span><span class='line'>Train Epoch: 2 [11800/60000 (20%)]  Loss: 0.2745484411716461
</span><span class='line'>Train Epoch: 2 [17800/60000 (30%)]  Loss: 0.29497388005256653
</span><span class='line'>Train Epoch: 2 [23800/60000 (40%)]  Loss: 0.30289018154144287
</span><span class='line'>Train Epoch: 2 [29800/60000 (50%)]  Loss: 0.3070503771305084
</span><span class='line'>Train Epoch: 2 [35800/60000 (60%)]  Loss: 0.34184956550598145
</span><span class='line'>Train Epoch: 2 [41800/60000 (70%)]  Loss: 0.25298836827278137
</span><span class='line'>Train Epoch: 2 [47800/60000 (80%)]  Loss: 0.2620333731174469
</span><span class='line'>Train Epoch: 2 [53800/60000 (90%)]  Loss: 0.2306962013244629
</span><span class='line'>Train Epoch: 2 [59800/60000 (100%)] Loss: 0.3607088327407837
</span><span class='line'>Train Epoch: 3 [5800/60000 (10%)] Loss: 0.22903680801391602
</span><span class='line'>Train Epoch: 3 [11800/60000 (20%)]  Loss: 0.20266905426979065
</span><span class='line'>Train Epoch: 3 [17800/60000 (30%)]  Loss: 0.25112783908843994
</span><span class='line'>Train Epoch: 3 [23800/60000 (40%)]  Loss: 0.22732162475585938
</span><span class='line'>Train Epoch: 3 [29800/60000 (50%)]  Loss: 0.2040642946958542
</span><span class='line'>Train Epoch: 3 [35800/60000 (60%)]  Loss: 0.20411935448646545
</span><span class='line'>Train Epoch: 3 [41800/60000 (70%)]  Loss: 0.19729731976985931
</span><span class='line'>Train Epoch: 3 [47800/60000 (80%)]  Loss: 0.24248278141021729
</span><span class='line'>Train Epoch: 3 [53800/60000 (90%)]  Loss: 0.21713067591190338
</span><span class='line'>Train Epoch: 3 [59800/60000 (100%)] Loss: 0.21899420022964478
</span><span class='line'>Train Epoch: 4 [5800/60000 (10%)] Loss: 0.1833008974790573
</span><span class='line'>Train Epoch: 4 [11800/60000 (20%)]  Loss: 0.1431916356086731
</span><span class='line'>Train Epoch: 4 [17800/60000 (30%)]  Loss: 0.2693891227245331
</span><span class='line'>Train Epoch: 4 [23800/60000 (40%)]  Loss: 0.21619722247123718
</span><span class='line'>Train Epoch: 4 [29800/60000 (50%)]  Loss: 0.22982168197631836
</span><span class='line'>Train Epoch: 4 [35800/60000 (60%)]  Loss: 0.26103365421295166
</span><span class='line'>Train Epoch: 4 [41800/60000 (70%)]  Loss: 0.23812612891197205
</span><span class='line'>Train Epoch: 4 [47800/60000 (80%)]  Loss: 0.25694453716278076
</span><span class='line'>Train Epoch: 4 [53800/60000 (90%)]  Loss: 0.2516362965106964
</span><span class='line'>Train Epoch: 4 [59800/60000 (100%)] Loss: 0.19944973289966583
</span><span class='line'>Train Epoch: 5 [5800/60000 (10%)] Loss: 0.20240060985088348
</span><span class='line'>Train Epoch: 5 [11800/60000 (20%)]  Loss: 0.17876243591308594
</span><span class='line'>Train Epoch: 5 [17800/60000 (30%)]  Loss: 0.1623563915491104
</span><span class='line'>Train Epoch: 5 [23800/60000 (40%)]  Loss: 0.14842358231544495
</span><span class='line'>Train Epoch: 5 [29800/60000 (50%)]  Loss: 0.1817866861820221
</span><span class='line'>Train Epoch: 5 [35800/60000 (60%)]  Loss: 0.1474132388830185
</span><span class='line'>Train Epoch: 5 [41800/60000 (70%)]  Loss: 0.20310136675834656
</span><span class='line'>Train Epoch: 5 [47800/60000 (80%)]  Loss: 0.2332427054643631
</span><span class='line'>Train Epoch: 5 [53800/60000 (90%)]  Loss: 0.14285261929035187
</span><span class='line'>Train Epoch: 5 [59800/60000 (100%)] Loss: 0.16876384615898132
</span><span class='line'>Train Epoch: 6 [5800/60000 (10%)] Loss: 0.15261796116828918
</span><span class='line'>Train Epoch: 6 [11800/60000 (20%)]  Loss: 0.12889760732650757
</span><span class='line'>Train Epoch: 6 [17800/60000 (30%)]  Loss: 0.17289716005325317
</span><span class='line'>Train Epoch: 6 [23800/60000 (40%)]  Loss: 0.12383649498224258
</span><span class='line'>Train Epoch: 6 [29800/60000 (50%)]  Loss: 0.14670918881893158
</span><span class='line'>Train Epoch: 6 [35800/60000 (60%)]  Loss: 0.20221909880638123
</span><span class='line'>Train Epoch: 6 [41800/60000 (70%)]  Loss: 0.21042735874652863
</span><span class='line'>Train Epoch: 6 [47800/60000 (80%)]  Loss: 0.16009800136089325
</span><span class='line'>Train Epoch: 6 [53800/60000 (90%)]  Loss: 0.23655852675437927
</span><span class='line'>Train Epoch: 6 [59800/60000 (100%)] Loss: 0.1507928967475891
</span><span class='line'>Train Epoch: 7 [5800/60000 (10%)] Loss: 0.16259260475635529
</span><span class='line'>Train Epoch: 7 [11800/60000 (20%)]  Loss: 0.14753393828868866
</span><span class='line'>Train Epoch: 7 [17800/60000 (30%)]  Loss: 0.15315169095993042
</span><span class='line'>Train Epoch: 7 [23800/60000 (40%)]  Loss: 0.1445481777191162
</span><span class='line'>Train Epoch: 7 [29800/60000 (50%)]  Loss: 0.132000133395195
</span><span class='line'>Train Epoch: 7 [35800/60000 (60%)]  Loss: 0.09024682641029358
</span><span class='line'>Train Epoch: 7 [41800/60000 (70%)]  Loss: 0.15115217864513397
</span><span class='line'>Train Epoch: 7 [47800/60000 (80%)]  Loss: 0.1311248242855072
</span><span class='line'>Train Epoch: 7 [53800/60000 (90%)]  Loss: 0.10016605257987976
</span><span class='line'>Train Epoch: 7 [59800/60000 (100%)] Loss: 0.12980304658412933
</span><span class='line'>Train Epoch: 8 [5800/60000 (10%)] Loss: 0.12055137008428574
</span><span class='line'>Train Epoch: 8 [11800/60000 (20%)]  Loss: 0.0916302502155304
</span><span class='line'>Train Epoch: 8 [17800/60000 (30%)]  Loss: 0.14020071923732758
</span><span class='line'>Train Epoch: 8 [23800/60000 (40%)]  Loss: 0.1205180287361145
</span><span class='line'>Train Epoch: 8 [29800/60000 (50%)]  Loss: 0.1326950639486313
</span><span class='line'>Train Epoch: 8 [35800/60000 (60%)]  Loss: 0.19008560478687286
</span><span class='line'>Train Epoch: 8 [41800/60000 (70%)]  Loss: 0.1668577790260315
</span><span class='line'>Train Epoch: 8 [47800/60000 (80%)]  Loss: 0.1530739814043045
</span><span class='line'>Train Epoch: 8 [53800/60000 (90%)]  Loss: 0.13995856046676636
</span><span class='line'>Train Epoch: 8 [59800/60000 (100%)] Loss: 0.05448155477643013
</span><span class='line'>Train Epoch: 9 [5800/60000 (10%)] Loss: 0.09004207700490952
</span><span class='line'>Train Epoch: 9 [11800/60000 (20%)]  Loss: 0.08474709838628769
</span><span class='line'>Train Epoch: 9 [17800/60000 (30%)]  Loss: 0.12305212020874023
</span><span class='line'>Train Epoch: 9 [23800/60000 (40%)]  Loss: 0.08823414891958237
</span><span class='line'>Train Epoch: 9 [29800/60000 (50%)]  Loss: 0.07393839210271835
</span><span class='line'>Train Epoch: 9 [35800/60000 (60%)]  Loss: 0.11944027245044708
</span><span class='line'>Train Epoch: 9 [41800/60000 (70%)]  Loss: 0.086588054895401
</span><span class='line'>Train Epoch: 9 [47800/60000 (80%)]  Loss: 0.08738173544406891
</span><span class='line'>Train Epoch: 9 [53800/60000 (90%)]  Loss: 0.1982622593641281
</span><span class='line'>Train Epoch: 9 [59800/60000 (100%)] Loss: 0.11448238044977188
</span><span class='line'>Train Epoch: 10 [5800/60000 (10%)]  Loss: 0.0850382074713707
</span><span class='line'>Train Epoch: 10 [11800/60000 (20%)] Loss: 0.06795746088027954
</span><span class='line'>Train Epoch: 10 [17800/60000 (30%)] Loss: 0.1085655689239502
</span><span class='line'>Train Epoch: 10 [23800/60000 (40%)] Loss: 0.05534665286540985
</span><span class='line'>Train Epoch: 10 [29800/60000 (50%)] Loss: 0.09382271021604538
</span><span class='line'>Train Epoch: 10 [35800/60000 (60%)] Loss: 0.026057429611682892
</span><span class='line'>Train Epoch: 10 [41800/60000 (70%)] Loss: 0.07035879790782928
</span><span class='line'>Train Epoch: 10 [47800/60000 (80%)] Loss: 0.1626715511083603
</span><span class='line'>Train Epoch: 10 [53800/60000 (90%)] Loss: 0.08846493810415268
</span><span class='line'>Train Epoch: 10 [59800/60000 (100%)]  Loss: 0.07113683223724365
</span><span class='line'>
</span><span class='line'>Test set: Average loss: 0.2453, Accuracy: 9204/10000 (92.04%)</span></code></pre></td></tr></table></div></figure>


<p>可以看到现在准确率达到了92.04%，可见适当增加卷积层可以提高识别精度</p>

<h4>4、预测</h4>

<p>使用命令预测一下</p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
</pre></td><td class='code'><pre><code class=''><span class='line'> python fashion_mnist_conv_net.py predict</span></code></pre></td></tr></table></div></figure>


<p><img src="http://blog.1nongfu.com/16871595198847.jpg" alt="" /></p>

<figure class='code'><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class='line-number'>1</span>
<span class='line-number'>2</span>
</pre></td><td class='code'><pre><code class=''><span class='line'>实际结果: ['裙子', '外套', '凉鞋', '包', '包', '毛衣', '牛仔裤', '运动鞋']
</span><span class='line'>推理结果: ['裙子', '外套', '凉鞋', '包', '包', '毛衣', '牛仔裤', '运动鞋']
</span></code></pre></td></tr></table></div></figure>



</div>


  <footer>
    <p class="meta">
      
  

<span class="byline author vcard">Posted by <span class="fn">Peter Xu</span></span>

      




<time class='entry-date' datetime='2023-07-27T16:31:49+08:00'><span class='date'><span class='date-month'>Jul</span> <span class='date-day'>27</span><span class='date-suffix'>th</span>, <span class='date-year'>2023</span></span> <span class='time'>4:31 pm</span></time>
      


    </p>
    
      <div class="sharing">
  
  
  
</div>

    
    <p class="meta">
      
        <a class="basic-alignment left" href="/blog/2023/05/26/pytorch-was-used-to-build-a-cnn-network-to-train-the-mnist-dataset/" title="Previous Post: 使用PyTorch构建CNN网络训练MNIST数据集">&laquo; 使用PyTorch构建CNN网络训练MNIST数据集</a>
      
      
    </p>
  </footer>
</article>



  <section>
    <h1>Comments</h1>
    <div id="comments" aria-live="polite"><!-- 多说评论框 start -->
	<div class="ds-thread" data-title="使用PyTorch构建CNN网络训练FashionMNIST数据集"></div>
<!-- 多说评论框 end -->
<!-- 多说公共JS代码 start (一个网页只需插入一次) -->
<script type="text/javascript">
var duoshuoQuery = {short_name:"xuyao91"};
	(function() {
		var ds = document.createElement('script');
		ds.type = 'text/javascript';ds.async = true;
		ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
		ds.charset = 'UTF-8';
		(document.getElementsByTagName('head')[0] 
		 || document.getElementsByTagName('body')[0]).appendChild(ds);
	})();
	</script>
<!-- 多说公共JS代码 end --></div>
  </section>


</div>

<aside class="sidebar">
  
    <section>
  <h1>Recent Posts</h1>
  <ul id="recent_posts">
    
      <li class="post">
        <a href="/blog/2023/07/27/pytorch-was-used-to-build-a-cnn-network-to-train-the-fashion-mnist-dataset/">使用PyTorch构建CNN网络训练FashionMNIST数据集</a>
      </li>
    
      <li class="post">
        <a href="/blog/2023/05/26/pytorch-was-used-to-build-a-cnn-network-to-train-the-mnist-dataset/">使用PyTorch构建CNN网络训练MNIST数据集</a>
      </li>
    
      <li class="post">
        <a href="/blog/2023/03/10/how-to-install-frp-vpn/">安装frp内网穿透工具</a>
      </li>
    
      <li class="post">
        <a href="/blog/2022/04/18/the-version-of-gloang-project/">Golang项目中的版本管理</a>
      </li>
    
      <li class="post">
        <a href="/blog/2020/12/31/one-last-little-surprise-for-2020/">2020年最后的一个小惊喜</a>
      </li>
    
  </ul>
</section>





  
</aside>


    </div>
  </div>
  <footer role="contentinfo"><p>
  Copyright &copy; 2023 - Peter Xu -
  <span class="credit">Powered by <a href="http://octopress.org">Octopress</a></span>
</p>

</footer>
  











</body>
</html>
